# -*- coding: utf-8 -*-


"""
Author: Yuntong Zhang

"""

"""Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/177Wn9w85wAh2tTNPW6hVCE4dinHi5oOq

### Data Processing
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd
import numpy as np

start_year = 1992
end_year = 2022
years_range = list(range(start_year, end_year + 1))

methane_df = pd.read_csv("/content/drive/My Drive/Research/EPI/epi2024_processeddata/CH4_raw_na.csv")
country = methane_df["country"].values # list of countries in total


##### CHANGE THE CODE
# get all independent variables from 1992 - 2022, add median to missing values
def get_country_data(years_range, country_name):
    folder_path = "/Users/jenniferzhang/Desktop/Risk Lab Research/epi2024_processeddata"
    # get the list of files in the folder
    file_names = os.listdir(folder_path)
    # print(file_names)
    column_names = ['Abbreviation'] + years_range
    # print(column_names)
    # create a new dataframe for each country
    country_df = pd.DataFrame(columns=column_names)

    # iterate through features
    for file in file_names:
        if file in ["CH4_raw_na.csv", "PRS_na.csv", "PRS_raw_na.csv"]:
            continue
        file_path = os.path.join(folder_path, file)
        # print(file_path)
        df = pd.read_csv(file_path)
        # print(df)
        columns = df.columns.tolist()
        columns[3:] = [col[-4:] for col in columns[3:]]
        df.columns = columns
        # start year for each feature
        starting_year = int(df.columns[3])
        # not including features that only have values after 2022
        if starting_year > end_year:
            print(f"Starting year {starting_year} not found in country_df columns.")
            continue
        # print(starting_year)
        for i, item in df.iterrows():
            if item["country"] == country_name:
                if file[:3] not in country_df['Abbreviation'].values:
                    # Add a new row with the file abbreviation
                    new_row = pd.Series([file[:3]] + [None] * (len(column_names) - 1), index=column_names)
                    # start index of the df
                    if starting_year <= years_range[0]:
                        start_index = df.columns.get_loc(str(years_range[0]))
                    else:
                        start_index = 3
                    values_to_insert = item[start_index:].values
                    #print(len(values_to_insert))
                    if len(values_to_insert) == 0:
                        continue
                    # start index of the df
                    if starting_year <= years_range[0]:
                        starting_index = 1
                    else:
                        starting_index = country_df.columns.get_loc(starting_year)
                    #print(starting_index)
                    ending_index = starting_index + len(values_to_insert)
                    if ending_index > len(new_row):
                        ending_index = len(new_row)
                    #print(ending_index)
                    # insert values into the new row
                    # Ensure that the length of values_to_insert fits the slice
                    if len(values_to_insert) > (ending_index - starting_index):
                        values_to_insert = values_to_insert[:(ending_index - starting_index)]

                    try:
                        new_row.iloc[starting_index:ending_index] = values_to_insert
                    except Exception as e:
                        print(f"Error during insertion: {e}")
                        print(f"Values to insert: {values_to_insert}")
                        print(f"Slice start_index:end_index: {starting_index}:{ending_index}")
                        print(f"New row after error: {new_row}")
                        raise

                    # print(new_row)
                    new_row_df = new_row.to_frame().transpose()
                    country_df = pd.concat([country_df, new_row_df], ignore_index=True)
                    #print(country_df)
                break

    country_df = country_df.replace({None: pd.NA})
    # Convert relevant columns to numeric
    for col in country_df.columns[1:]:  # Exclude 'Abbreviation' column
        country_df[col] = pd.to_numeric(country_df[col], errors='coerce')
    numerical_df = country_df.select_dtypes(include='number')
    medians = numerical_df.median()
    # Fill NaN values with the median of each numeric column
    country_df[numerical_df.columns] = numerical_df.fillna(medians)
    country_df_transposed = country_df.set_index('Abbreviation').transpose()
    return country_df_transposed

def input_data_to_excel(country_list):
    start_year = 1992
    end_year = 2022
    years_range = list(range(start_year, end_year + 1))
    for c in country_list:
        country_df_transposed = get_country_data(years_range, c)
        file_path = f"/Users/jenniferzhang/Desktop/Risk Lab Research/epi2024_countrydata/{c}.xlsx"
        country_df_transposed.to_excel(file_path, index=False)

# input the data into excel
# input_data_to_excel(country)

# Transfer methane dataframe
for col in methane_df.columns[3:]:
    methane_df[col] = pd.to_numeric(methane_df[col], errors='coerce')
    num_df = methane_df.select_dtypes(include='number')
    medians = num_df.median()
    # Fill NaN values with the median of each numeric column
    methane_df[num_df.columns] = num_df.fillna(medians)

methane_df_last_31 = methane_df.iloc[:, -31:]
methane_df_3 = methane_df.iloc[:, 2]
methane_df = pd.concat([methane_df_3, methane_df_last_31], axis=1)
methane_df.head()

from scipy import stats
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.model_selection import train_test_split
from statsmodels.regression.linear_model import OLS
from zipfile import BadZipFile

"""### Check Stationary for Y"""

def test_stationarity(series):
    result = adfuller(series)
    p_value = result[1]  # p-value from the ADF test
    return p_value

methane_df_t = methane_df.T
methane_df_t.columns = methane_df_t.iloc[0]
methane_df_t = methane_df_t.drop(methane_df_t.index[0])

# print(methane_df_t.head())

columns_to_drop = []
# Apply the ADF test to each row and store results
for i in range(len(methane_df_t.columns)):
    if test_stationarity(methane_df_t.iloc[:, i]) > 0.5:
        column_name = methane_df_t.columns[i]
        # print(column_name)
        methane_df_t[column_name] = pd.to_numeric(methane_df_t.iloc[:, i], errors='coerce')

        # Create cube root and differenced columns
        methane_df_t[f'{column_name}_cbrt'] = np.cbrt(methane_df_t.iloc[:, i])
        methane_df_t[f'{column_name}_cbrt_diff'] = methane_df_t[f'{column_name}_cbrt'].diff()
        first_value = methane_df_t[f'{column_name}_cbrt_diff'].iloc[1]
        methane_df_t[f'{column_name}_cbrt_diff'] = methane_df_t[f'{column_name}_cbrt_diff'].fillna(first_value)
        columns_to_drop.append(column_name)
        columns_to_drop.append(f'{column_name}_cbrt')

methane_df_t.drop(columns=columns_to_drop, axis=1, inplace=True)

methane_df_t

methane_df_t.shape

"""### Check Stationary for X"""

sample = "/Users/jenniferzhang/Desktop/Risk Lab Research/epi2024_countrydata/Afghanistan.xlsx"
sample_df = pd.read_excel(sample)
feature = list(sample_df.columns)

# Define the folder path and retrieve file names
folder_path_country = "/Users/jenniferzhang/Desktop/Risk Lab Research/epi2024_countrydata/"
file_names_country = os.listdir(folder_path_country)

# Initialize an empty DataFrame to store ADF p-values
adf_results = pd.DataFrame(columns=feature)

for file in file_names_country:
    file_path = os.path.join(folder_path_country, file)

    try:
        # Attempt to read the Excel file
        df_country = pd.read_excel(file_path, engine='openpyxl')
    except BadZipFile:
        print(f"Skipping file {file}: Not a valid zip file")
        continue
    except Exception as e:
        print(f"Error reading {file}: {e}")
        continue

    # List to store p-values for the current file (country)
    adf_p = []
    country_name = os.path.splitext(file)[0]

    for column in df_country.columns:
        try:
            # Check if the column is constant
            if df_country[column].nunique() <= 1:
                print(f"Skipping column {column} in {file}: Column is constant")
                adf_p.append(None)  # Append None if the column is constant
                continue

            # Apply ADF test and store the p-value
            p_value = adfuller(df_country[column])[1]
            adf_p.append(p_value)

            # If p-value > 0.5, apply differencing
            if p_value > 0.5:
                df_country[f'{column}_cbrt'] = np.cbrt(df_country[column])
                df_country[f'{column}_cbrt_diff'] = df_country[f'{column}_cbrt'].diff()
                first_value = df_country[f'{column}_cbrt_diff'].iloc[1]
                df_country[f'{column}_cbrt_diff'] = df_country[f'{column}_cbrt_diff'].fillna(first_value)
        except ValueError as ve:
            print(f"Skipping column {column} in {file}: {ve}")
            adf_p.append(None)  # Append None if there is a ValueError
            continue

    # Add the country name and p-values to the DataFrame
    adf_results.loc[country_name] = adf_p

    # Group columns by the first three characters
    grouped_cols = df_country.columns.to_series().groupby(df_country.columns.to_series().str[:3])

    # Identify columns to keep
    columns_to_keep = []
    for group, cols in grouped_cols:
        # If there's only one column in the group, keep it
        if len(cols) == 1:
            columns_to_keep.append(cols[0])
        else:
            # Filter out columns that end with '_cbrt_diff' if there are multiple columns
            cbrt_diff_cols = [col for col in cols if col.endswith('_cbrt_diff')]
            if cbrt_diff_cols:
                columns_to_keep.append(cbrt_diff_cols[0])

    df_filtered = df_country[columns_to_keep]

    file_path_new = f"/Users/jenniferzhang/Desktop/Risk Lab Research/epi2024_countrydata_new/{country_name}.xlsx"
    df_filtered.to_excel(file_path_new, index=False)

# Display the first few rows of the results
# print(adf_results.head())

adf_results_true_false = adf_results <= 0.5
adf_results_true_false.head()

"""### Feature Importance Models

#### Lasso
"""

from sklearn.linear_model import LassoCV

def lasso(x_train_country, y_train_country):
    model = LassoCV(cv=5).fit(x_train_country, y_train_country)
    importance = model.coef_.tolist()
    return importance

folder_path_country = "/Users/jenniferzhang/Desktop/Risk Lab Research/epi2024_countrydata_new/"
file_names_country = os.listdir(folder_path_country)

# for each country
lasso_df = pd.DataFrame(columns=feature)
for file in file_names_country:
    file_path = os.path.join(folder_path_country, file)
    try:
        # Attempt to read the Excel file
        df_country = pd.read_excel(file_path, engine='openpyxl')
    except BadZipFile:
        print(f"Skipping file {file}: Not a valid zip file")
        continue
    except Exception as e:
        print(f"Error reading {file}: {e}")
        continue
    country_name = os.path.splitext(file)[0]
    for c in methane_df_t.columns:
        if country_name in c:
            y_country = methane_df_t[c].values
            break
    X_train, X_test, y_train, y_test = train_test_split(df_country, y_country, test_size=0.2, random_state=42)
    feature_rank = lasso(X_train, y_train)
    new_row_df = pd.Series(feature_rank, index=feature)
    new_row_df = new_row_df.to_frame().T
    lasso_df = pd.concat([lasso_df, new_row_df], axis = 0,ignore_index=True)
lasso_df

lasso_feature_rank = abs(lasso_df.mean()).sort_values(ascending=False)
lasso_feature = list(lasso_feature_rank.head(30).index)
lasso_feature

"""#### Ridge"""

from sklearn.linear_model import RidgeCV

folder_path_country = "/Users/jenniferzhang/Desktop/Risk Lab Research/epi2024_countrydata_new/"
file_names_country = os.listdir(folder_path_country)

# Initialize DataFrame for Ridge results
ridge_df = pd.DataFrame(columns=feature)

for file in file_names_country:
    file_path = os.path.join(folder_path_country, file)
    try:
        # Attempt to read the Excel file
        df_country = pd.read_excel(file_path, engine='openpyxl')
    except BadZipFile:
        print(f"Skipping file {file}: Not a valid zip file")
        continue
    except Exception as e:
        print(f"Error reading {file}: {e}")
        continue

    country_name = os.path.splitext(file)[0]

    for c in methane_df_t.columns:
        if country_name in c:
            y_country = methane_df_t[c].values
            break

    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(df_country, y_country, test_size=0.2, random_state=42)

    # Fit Ridge regression model
    ridge_model = RidgeCV(alphas=[0.1, 1.0, 10.0], store_cv_values=True)  # Adjust alphas as needed
    ridge_model.fit(X_train, y_train)

    # Get the coefficients
    feature_rank = ridge_model.coef_

    # Convert to DataFrame and append
    new_row_df = pd.Series(feature_rank, index=feature)
    new_row_df = new_row_df.to_frame().T
    ridge_df = pd.concat([ridge_df, new_row_df], axis=0, ignore_index=True)

ridge_df

ridge_feature_rank = abs(ridge_df.mean()).sort_values(ascending=False)
ridge_feature = list(ridge_feature_rank.head(30).index)
ridge_feature

"""#### SHAP Value"""

from sklearn.linear_model import Lasso
import shap


folder_path_country = "/Users/jenniferzhang/Desktop/Risk Lab Research/epi2024_countrydata_new/"
file_names_country = os.listdir(folder_path_country)

# Initialize DataFrame for Shapley results
shap_df = pd.DataFrame(columns=feature)

for file in file_names_country:
    file_path = os.path.join(folder_path_country, file)
    try:
        # Attempt to read the Excel file
        df_country = pd.read_excel(file_path, engine='openpyxl')
    except BadZipFile:
        print(f"Skipping file {file}: Not a valid zip file")
        continue
    except Exception as e:
        print(f"Error reading {file}: {e}")
        continue

    country_name = os.path.splitext(file)[0]

    for c in methane_df_t.columns:
        if country_name in c:
            y_country = methane_df_t[c].values
            break

    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(df_country, y_country, test_size=0.2, random_state=42)

    X_train_100 = shap.utils.sample(X_train, 100)  # 100 instances for use as the background distribution

    # A simple linear model
    lasso = Lasso(alpha=0.1,max_iter=10000)
    lasso.fit(X_train, y_train)

    # Explain the model's predictions using SHAP
    explainer = shap.Explainer(lasso, X_train_100)
    shap_values = explainer(X_train) # (number of samples, number of features)

    # Compute shap value for each data point and take avg over the whole data set
    # number of avg = number of features

    shap_values_array = shap_values.values
    column_mean = np.mean(shap_values_array, axis=0)
    sorted_index = np.argsort(column_mean)
    sorted_features = X_train.columns[sorted_index] # feature names
    sorted_column_means = column_mean[sorted_index] # values
    new_row_df = pd.Series(sorted_column_means, index=sorted_features)
    new_row_df = new_row_df.to_frame().T
    shap_df = pd.concat([shap_df, new_row_df], ignore_index=True)

shap_df

shap_feature_rank = abs(shap_df.mean()).sort_values(ascending=False)
shap_feature = list(shap_feature_rank.head(30).index)
shap_feature

"""#### Added together"""

common_elements = set(lasso_feature) & set(ridge_feature) & set(shap_feature)
common_elements

"""Get top 20 features

Get common features
- If > 10 (>7), good
- If not, reduce feature importance methods

- 'BCA', Adjusted emissions growth rate for black carbon
- 'GHG', Greenhouse gas emissions
- 'GHN', GHG Net0 by 2050, Greenhouse Gas Net Emissions
- 'PMD', Ambient particulate matter pollution, Particulate Matter Density
- 'SO2', Sulfur Dioxide
- 'TCG', Net change in tree cover, Total Greenhouse Gases
- 'TCC', Tree cover loss, annual, Total Carbon Content
- 'FOG', F-gases, Fuel Oil Gas

Reasonable story why the common features are important for methane prediction (why relevent)

### Prediction Model

For each country extract the 8 features from the dataset as X to make prediction of methane emission Y.
"""

most_feature = ['BCA', 'GHG', 'GHN', 'PMD', 'SO2', 'TCG', 'TCC', 'FOG']

folder_path_country = "/Users/jenniferzhang/Desktop/Risk Lab Research/epi2024_countrydata/"
file_names_country = os.listdir(folder_path_country)

# for each country
for file in file_names_country:
    file_path = os.path.join(folder_path_country, file)
    try:
        # Attempt to read the Excel file
        df_country = pd.read_excel(file_path, engine='openpyxl')
    except BadZipFile:
        print(f"Skipping file {file}: Not a valid zip file")
        continue
    except Exception as e:
        print(f"Error reading {file}: {e}")
        continue
    country_name = os.path.splitext(file)[0]

    x_country = pd.DataFrame(columns=most_feature)
    for f in most_feature:
        if f in df_country.columns:
            x = df_country[f].values
            # add x to x_country
            x_country[f] = x

    file_path_new = f"/Users/jenniferzhang/Desktop/Risk Lab Research/epi2024_countrydata_features/{country_name}.xlsx"
    x_country.to_excel(file_path_new, index=False)

"""#### SVM"""

from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler

methane_df_tt = methane_df.T
methane_df_tt.columns = methane_df_tt.iloc[0]
methane_df_tt = methane_df_tt.drop(methane_df_tt.index[0])
methane_df_tt.head()

folder_path_country = "/content/drive/My Drive/Research/EPI/epi2024_countrydata_features/"
file_names_country = os.listdir(folder_path_country)

mse_list = []
r2_list = []

# for each country
for file in file_names_country:
    file_path = os.path.join(folder_path_country, file)
    try:
        # Attempt to read the Excel file
        X = pd.read_excel(file_path, engine='openpyxl')
        X = X.values
    except BadZipFile:
        print(f"Skipping file {file}: Not a valid zip file")
        continue
    except Exception as e:
        print(f"Error reading {file}: {e}")
        continue
    country_name = os.path.splitext(file)[0]

    for c in methane_df_tt.columns:
        if country_name in c:
            Y = methane_df_tt[c].values
            # print(type(Y))
            break

    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

    # Standardize the features
    scaler_X = StandardScaler()
    X_train = scaler_X.fit_transform(X_train)
    X_test = scaler_X.transform(X_test)

    # Standardize the target variable based on the training data
    scaler_y = StandardScaler()
    y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()
    y_test = scaler_y.transform(y_test.reshape(-1, 1)).ravel()  # Standardize y_test for evaluation

    # svm regression
    svm_regressor = SVR()

    # Define the parameter grid
    param_grid = {
        'C': [0.1, 1, 10, 100],        # Regularization parameter
        'kernel': ['linear', 'rbf', 'poly'],  # Kernel type
        'gamma': ['scale', 'auto'],    # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'
        'degree': [3, 4, 5]            # Degree of the polynomial kernel function (only relevant for 'poly')
    }

    # Use GridSearchCV to search for the best parameters
    grid_search = GridSearchCV(svm_regressor, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)
    grid_search.fit(X_train, y_train)


    # Get the best parameters and the best model
    best_params = grid_search.best_params_
    best_model = grid_search.best_estimator_

    print("Best Parameters:", best_params)

    # Evaluate the model on the test set
    y_pred = best_model.predict(X_test)

    # Train the classifier on the training data
    svm_regressor.fit(X_train, y_train)

    # Predict the labels for the test data
    y_pred = svm_regressor.predict(X_test)

    # Calculate regression metrics
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    mse_list.append(mse)
    r2_list.append(r2)

np.mean(mse_list)

np.mean(r2_list)

"""#### XGB"""

!pip install optuna

import optuna
import xgboost as xgb

# Define the objective function for Optuna
folder_path_country = "/content/drive/My Drive/Research/EPI/epi2024_countrydata_features/"
file_names_country = os.listdir(folder_path_country)

xgb_results = []

# for each country
for file in file_names_country:
    file_path = os.path.join(folder_path_country, file)
    try:
        # Attempt to read the Excel file
        X = pd.read_excel(file_path, engine='openpyxl')
        X = X.values
    except BadZipFile:
        print(f"Skipping file {file}: Not a valid zip file")
        continue
    except Exception as e:
        print(f"Error reading {file}: {e}")
        continue
    country_name = os.path.splitext(file)[0]

    for c in methane_df_tt.columns:
        if country_name in c:
            Y = methane_df_tt[c].values
            # print(type(Y))
            break

    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)
    # Standardize the features
    scaler_X = StandardScaler()
    X_train = scaler_X.fit_transform(X_train)
    X_test = scaler_X.transform(X_test)

    # Standardize the target variable based on the training data
    scaler_y = StandardScaler()
    y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()
    y_test = scaler_y.transform(y_test.reshape(-1, 1)).ravel()  # Standardize y_test for evaluation

    def xgb_objective(trial):
        # Define hyperparameters to tune
        param = {
            'objective': 'reg:squarederror',
            'booster': 'gbtree',
            'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),
            'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),
            'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.5, 0.7, 0.9, 1.0]),
            'subsample': trial.suggest_categorical('subsample', [0.5, 0.7, 0.9, 1.0]),
            'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'max_depth': trial.suggest_int('max_depth', 3, 9),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
            'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),
        }

        # Train the model
        model = xgb.XGBRegressor(**param)
        model.fit(X_train, y_train)

        # Predict and calculate RMSE
        y_pred = model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)

        return mse

    # Set up Optuna study
    study = optuna.create_study(direction='minimize')
    study.optimize(xgb_objective, n_trials=50, timeout=600)  # Adjust trials and timeout as needed

    # Train final model with best parameters
    best_params = study.best_trial.params
    model = xgb.XGBRegressor(**best_params)
    model.fit(X_train, y_train)

    # Predict and calculate metrics
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Store the results for this country
    xgb_results.append({
        'Country': country,
        'MSE': mse,
        'R2': r2
    })

# Create a DataFrame from the results
xgb_results_df = pd.DataFrame(xgb_results)

# Calculate the average MSE and R² score across all countries
xgb_average_mse = xgb_results_df['MSE'].mean()
xgb_average_r2 = xgb_results_df['R2'].mean()

xgb_average_r2

xgb_average_mse

"""#### LightBGM"""

import lightgbm as lgb
import optuna
from sklearn.model_selection import KFold
from math import sqrt

# Define the objective function for Optuna
folder_path_country = "/content/drive/My Drive/Research/EPI/epi2024_countrydata_features/"
file_names_country = os.listdir(folder_path_country)

lgb_results = []

# for each country
for file in file_names_country:
    file_path = os.path.join(folder_path_country, file)
    try:
        # Attempt to read the Excel file
        X = pd.read_excel(file_path, engine='openpyxl')
        X = X.values
    except BadZipFile:
        print(f"Skipping file {file}: Not a valid zip file")
        continue
    except Exception as e:
        print(f"Error reading {file}: {e}")
        continue
    country_name = os.path.splitext(file)[0]

    for c in methane_df_tt.columns:
        if country_name in c:
            Y = methane_df_tt[c].values
            # print(type(Y))
            break

    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)
    # Standardize the features
    scaler_X = StandardScaler()
    X_train = scaler_X.fit_transform(X_train)
    X_test = scaler_X.transform(X_test)

    # Standardize the target variable based on the training data
    scaler_y = StandardScaler()
    y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()
    y_test = scaler_y.transform(y_test.reshape(-1, 1)).ravel()  # Standardize y_test for evaluation

    def lgb_objective(trial):
        # Define hyperparameters to tune
        param = {
            'objective': 'regression',
            'metric': 'mse',  # Use MSE as the metric
            'verbosity': -1,
            'boosting_type': 'gbdt',
            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),
            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),
            'num_leaves': trial.suggest_int('num_leaves', 2, 256),
            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),
            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),
            'early_stopping_round': 50
        }

        # Train/Validation split within the training data
        X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

        # Create Dataset objects for LightGBM
        train_data = lgb.Dataset(X_train_, label=y_train_)
        val_data = lgb.Dataset(X_val, label=y_val)

        # Train the model with early stopping
        gbm = lgb.train(
            param,
            train_data,
            valid_sets=[val_data],
        )

        # Predict and calculate MSE on the validation set
        preds = gbm.predict(X_val, num_iteration=gbm.best_iteration)
        mse = mean_squared_error(y_val, preds)

        return mse


    # Set up Optuna study
    study = optuna.create_study(direction='minimize')
    study.optimize(lgb_objective, n_trials=50, timeout=600)  # Adjust trials and timeout as needed

    # Train final model with best parameters
    best_params = study.best_trial.params
    best_params['objective'] = 'regression'
    best_params['metric'] = 'mse'
    best_params['verbosity'] = -1
    model = lgb.train(best_params, lgb.Dataset(X_train, label=y_train), valid_sets=[lgb.Dataset(X_test, label=y_test)])

    # Predict and calculate metrics
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Store the results for this country
    lgb_results.append({
        'Country': country,
        'MSE': mse,
        'R2': r2
    })

# Create a DataFrame from the results
lgb_results_df = pd.DataFrame(lgb_results)

# Calculate the average MSE and R² score across all countries
lgb_average_mse = lgb_results_df['MSE'].mean()
lgb_average_r2 = lgb_results_df['R2'].mean()

lgb_average_r2

lgb_average_mse

"""#### Random Forest"""

from sklearn.ensemble import RandomForestRegressor

# Define the objective function for Optuna
folder_path_country = "/content/drive/My Drive/Research/EPI/epi2024_countrydata_features/"
file_names_country = os.listdir(folder_path_country)

rf_results = []

# for each country
for file in file_names_country:
    file_path = os.path.join(folder_path_country, file)
    try:
        # Attempt to read the Excel file
        X = pd.read_excel(file_path, engine='openpyxl')
        X = X.values
    except BadZipFile:
        print(f"Skipping file {file}: Not a valid zip file")
        continue
    except Exception as e:
        print(f"Error reading {file}: {e}")
        continue
    country_name = os.path.splitext(file)[0]

    for c in methane_df_tt.columns:
        if country_name in c:
            Y = methane_df_tt[c].values
            # print(type(Y))
            break

    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)
    # Standardize the features
    scaler_X = StandardScaler()
    X_train = scaler_X.fit_transform(X_train)
    X_test = scaler_X.transform(X_test)

    # Standardize the target variable based on the training data
    scaler_y = StandardScaler()
    y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()
    y_test = scaler_y.transform(y_test.reshape(-1, 1)).ravel()  # Standardize y_test for evaluation

    def rf_objective(trial):
        # Define hyperparameters to tune
        param = {
          'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
          'max_depth': trial.suggest_int('max_depth', 3, 20),
          'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
          'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),
          'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),
        }


        # Train the model
        model = RandomForestRegressor(**param, random_state=42)
        model.fit(X_train, y_train)

        # Predict and calculate RMSE
        y_pred = model.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred)

        return rmse

    # Set up Optuna study
    study = optuna.create_study(direction='minimize')
    study.optimize(rf_objective, n_trials=30, timeout=600)  # Adjust trials and timeout as needed

    # Train final model with best parameters
    best_params = study.best_trial.params
    model = RandomForestRegressor(**best_params, random_state=42)
    model.fit(X_train, y_train)

    # Predict and calculate metrics
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Store the results for this country
    rf_results.append({
        'Country': country,
        'MSE': mse,
        'R2': r2
    })

# Create a DataFrame from the results
rf_results_df = pd.DataFrame(rf_results)

# Calculate the average MSE and R² score across all countries
rf_average_mse = rf_results_df['MSE'].mean()
rf_average_r2 = rf_results_df['R2'].mean()

rf_average_mse

rf_average_r2

"""## Trend for each Country"""

methane_df_t.head()

import matplotlib.pyplot as plt

methane = pd.DataFrame()

for c in methane_df_t.columns:
    m = methane_df_t[c].values
    m = np.array(m)

    methane[c] = m

# Calculate the decrease for each country (last year - first year)
decrease = {}
for c in methane.columns:
    first_year = methane[c].values[0]
    last_year = methane[c].values[-1]
    decrease[c] = first_year - last_year

top_decreased_countries = sorted(decrease.items(), key=lambda x: x[1], reverse=True)[:10]
top_countries = [country for country, _ in top_decreased_countries]

years = np.arange(1992, 2023)

# Plot normalized values for the top 10 countries
plt.figure(figsize=(14, 8))
for country in top_countries:
    plt.plot(years, methane[country], label=country)

plt.xlabel('Years')
plt.ylabel('Value')
plt.title('Values for Top 10 Countries')
plt.legend()
plt.show()

top_decreased_countries = sorted(decrease.items(), key=lambda x: x[1], reverse=False)[:10]
top_countries = [country for country, _ in top_decreased_countries]

years = np.arange(1992, 2023)

# Plot normalized values for the top 10 countries
plt.figure(figsize=(14, 8))
for country in top_countries:
    plt.plot(years, methane[country], label=country)

plt.xlabel('Years')
plt.ylabel('Value')
plt.title('Values for Top 10 Countries')
plt.legend()
plt.show()

normalized_df = pd.DataFrame()

for c in methane_df_t.columns:
    m = methane_df_t[c].values
    m = np.array(m)

    norm_m = (m - m.min()) / (m.max() - m.min())
    normalized_df[c] = norm_m

# Calculate the decrease for each country (last year - first year)
decrease = {}
for c in normalized_df.columns:
    first_year = normalized_df[c].values[0]
    last_year = normalized_df[c].values[-1]
    decrease[c] = first_year - last_year

top_decreased_countries = sorted(decrease.items(), key=lambda x: x[1], reverse=True)[:10]
top_countries = [country for country, _ in top_decreased_countries]

years = np.arange(1992, 2023)

# Plot normalized values for the top 10 countries
plt.figure(figsize=(14, 8))
for country in top_countries:
    plt.plot(years, normalized_df[country], label=country)

plt.xlabel('Years')
plt.ylabel('Normalized Value')
plt.title('Normalized Values for Top 10 Countries')
plt.legend()
plt.show()

top_decreased_countries = sorted(decrease.items(), key=lambda x: x[1], reverse=False)[:10]
top_countries = [country for country, _ in top_decreased_countries]

years = np.arange(1992, 2023)

# Plot normalized values for the top 10 countries
plt.figure(figsize=(14, 8))
for country in top_countries:
    plt.plot(years, normalized_df[country], label=country)

plt.xlabel('Years')
plt.ylabel('Normalized Value')
plt.title('Normalized Values for Top 10 Countries')
plt.legend()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Define the years from 1992 to 2022
years = np.arange(1992, 2023)

# Normalize each country's data
methane = pd.DataFrame()
for c in methane_df_t.columns:
    m = methane_df_t[c].values
    m = np.array(m)
    # Store normalized data in a new DataFrame
    methane[c] = m

# Calculate the decrease and steadiness for each country
decrease = {}
steadiness = {}

for c in methane.columns:
    values = methane[c].values
    first_year = values[0]
    last_year = values[-1]

    # Calculate total decrease
    total_decrease = first_year - last_year
    decrease[c] = total_decrease

    # Calculate steadiness: proportion of consecutive decreases
    consecutive_decreases = np.sum(np.diff(values) < 0)
    steadiness_proportion = consecutive_decreases / (len(values) - 1)
    steadiness[c] = steadiness_proportion

# Combine decrease and steadiness and sort by both
decrease_steadiness = {c: (decrease[c], steadiness[c]) for c in methane.columns}
sorted_countries = sorted(decrease_steadiness.items(), key=lambda x: (x[1][0], x[1][1]), reverse=True)

# Filter top 10 countries with most decrease and high steadiness
top_countries = [country for country, (dec, stead) in sorted_countries if stead > 0.8][:10]

# Plot normalized values for the top 10 countries
plt.figure(figsize=(14, 8))
for country in top_countries:
    plt.plot(years, methane[country], label=country)

# Add labels and titles
plt.title('Top 5 Countries with the Greatest and Steady Decrease in Methane Emissions')
plt.xlabel('Year')
plt.ylabel('Methane Emissions')
plt.xticks(years[::2], rotation=45)  # Show every second year for readability
plt.legend(loc='upper right')
plt.grid(True)
plt.tight_layout()
plt.show()

# Print the top 10 countries with the most decrease and high steadiness
print("Top 10 countries with the greatest and steady decrease in methane emissions:")
for country in top_countries:
    print(f"{country}: Decrease={decrease[country]:.2f}, Steadiness={steadiness[country]:.2f}")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Define the years from 1992 to 2022
years = np.arange(1992, 2023)

# Normalize each country's data
normalized_df = pd.DataFrame()
for c in methane_df_t.columns:
    m = methane_df_t[c].values
    m = np.array(m)

    # Min-max normalization for each country's data
    norm_m = (m - m.min()) / (m.max() - m.min())

    # Store normalized data in a new DataFrame
    normalized_df[c] = norm_m

# Calculate the decrease and steadiness for each country
decrease = {}
steadiness = {}

for c in normalized_df.columns:
    values = normalized_df[c].values
    first_year = values[0]
    last_year = values[-1]

    # Calculate total decrease
    total_decrease = first_year - last_year
    decrease[c] = total_decrease

    # Calculate steadiness: proportion of consecutive decreases
    consecutive_decreases = np.sum(np.diff(values) < 0)
    steadiness_proportion = consecutive_decreases / (len(values) - 1)
    steadiness[c] = steadiness_proportion

# Combine decrease and steadiness and sort by both
decrease_steadiness = {c: (decrease[c], steadiness[c]) for c in normalized_df.columns}
sorted_countries = sorted(decrease_steadiness.items(), key=lambda x: (x[1][0], x[1][1]), reverse=True)

# Filter top 10 countries with most decrease and high steadiness
top_countries = [country for country, (dec, stead) in sorted_countries if stead > 0.8][:10]

# Plot normalized values for the top 10 countries
plt.figure(figsize=(14, 8))
for country in top_countries:
    plt.plot(years, normalized_df[country], label=country)

# Add labels and titles
plt.title('Top 5 Countries with the Greatest and Steady Decrease in Methane Emissions (Normalized)')
plt.xlabel('Year')
plt.ylabel('Normalized Methane Emissions')
plt.xticks(years[::2], rotation=45)  # Show every second year for readability
plt.legend(loc='upper right')
plt.grid(True)
plt.tight_layout()
plt.show()

# Print the top 10 countries with the most decrease and high steadiness
print("Top 10 countries with the greatest and steady decrease in methane emissions:")
for country in top_countries:
    print(f"{country}: Decrease={decrease[country]:.2f}, Steadiness={steadiness[country]:.2f}")

methane_sum = methane_df_t.sum(axis=1)
# Plot the global sum of methane emissions over time
plt.figure(figsize=(12, 6))
plt.plot(years, methane_sum, label='Global Sum of Methane Emissions', color='blue')
plt.title('Global Methane Emissions (Sum) Over Time')
plt.xlabel('Year')
plt.ylabel('Methane Emissions (Sum)')
plt.grid(True)
plt.show()

# Calculate the difference for each country from 1992 to 2022
differences = methane_df_t.iloc[-1] - methane_df_t.iloc[0]  # Last year - first year

# Count the number of countries that have decreased and increased
decreased_count = (differences <= 0).sum()
increased_count = (differences > 0).sum()

print(f"Number of countries with decreased methane levels: {decreased_count}")
print(f"Number of countries with increased methane levels: {increased_count}")
